{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Теоретические задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Ответы в листьях регрессионного дерева"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какая стратегия поведения в листьях регрессионного дерева приводит к меньшему матожиданию ошибки по MSE: отвечать средним значением таргета на объектах обучающей выборки,\n",
    "попавших в лист, или отвечать таргетом для случайного объекта из листа (считая все объекты\n",
    "равновероятными)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение. \n",
    "Пусть на тестовой выборке в лист попало k объектов, значения целевых переменных которых $ y_i $, а ответ выдаваемый обученной модель $ y $. Предполагаем, что все объекты имеют одинаковое значение целевой переменной: $y_i = y_ 1,      \\forall  i = 2, ..., k $.\n",
    "\n",
    "Матожидание ошибки в случае ответа средним значением по листу: \n",
    "$$E (MSE_{mean}) = E (y - \\frac {\\sum_{i=1}^k y_i}{k})^2  -> (y - \\frac {\\sum_{i=1}^k y_i}{k})^2 = (\\frac {1}{k}\\cdot \\sum_{i=1}^k {(y - y_i}))^2 =  (\\frac {\\cdot \\sum_{i=1}^k {(y - y_i})} {k})^2(1)  $$\n",
    "\n",
    "Неравенство о средних $  \\frac {\\sqrt{\\sum_{i=1}^k {y_i ^2}}} {k} \\geq \\frac {\\sum_{i=1}^k {y_i}} {k}   \\Rightarrow \\frac {\\sum_{i=1}^k {y_i ^2}} {k^2} \\geq  \\frac {(\\sum_{i=1}^k {y_i})^2} {k^2} $\n",
    "\n",
    "Тогда: $$ (1) \\leq \\frac { \\sum_{i=1}^k {(y - y_i)^2}} {k^2} \\leq \\frac {\\cdot \\sum_{i=1}^k {(y - y_i)^2}} {k} = E (y - y_i) = E_{random} $$\n",
    " \n",
    "Получили, что к меньшему матожиданию ошибки по MSE приводит ответ в листе равный среднему значению целевой функции на объектах обучающей выборки. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Линейные модели в деревьях\n",
    "\n",
    "Одна из частых идей - попытаться улучшить регрессионное дерево, выдавая вместо константных ответов в листьях ответ линейной регрессии, обученной на объектах из этого листа. Как правило такая стратегия не дает никакого ощутимого выигрыша. Попробуйте объяснить, почему? Как стоит модифицировать построение разбиений в дереве по MSE, чтобы при\n",
    "разбиении получались множества, на которых линейные модели должны работать неплохо?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Unsupervised decision tree\n",
    "\n",
    "Unsupervised решающие деревья можно было бы применить для кластеризации выборки или\n",
    "оценки плотности, но проблема построения таких деревьев заключается в введении меры\n",
    "информативности. В одной статье предлагался следующий подход - оценивать энтропию\n",
    "множества S по формуле: $$ H(S) = \\frac {1} {2} \\cdot ln ((2\\pi e)^n |\\Sigma|) $$\n",
    "\n",
    "Здесь $ \\sum $ -  оцененная по множеству матрица ковариаций. Т.е. не имея других сведений, в\n",
    "предложенном подходе мы по умолчанию считаем, что скопления точек можно приближенно\n",
    "считать распределенными нормально. Убедитесь, что это выражение в самом деле задает\n",
    "энтропию многомерного нормального распределения.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение. Плотность многомерного нормального распределения задается формулой: $$ p(x) = \\frac {1} {(2 \\cdot \\pi) ^ (\\frac {2}{n}) \\cdot \\sqrt {|\\Sigma|}} \\cdot e ^ { - \\frac {1} {2} \\cdot (x - \\mu)^T \\cdot \\Sigma ^ {-1} \\cdot (x - \\mu)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Информационная энтропия Шеннона: $ H = - \\sum_{i = 1} ^k {p_i \\cdot \\ln {(p_i)}} $\n",
    "\n",
    "Тогда энтропия нормального многомерного распределения: \n",
    "$$ H(p) = - \\int {...} \\int_{R_n} {p(x) \\cdot ln p(x)  dx} = $$ \n",
    "$$= \\int {...} \\int_{R_n} p(x) \\cdot \\Bigl( \\frac {1}{2} \\cdot (x - \\mu)^T \\cdot (\\Sigma)^{-1} \\cdot (x - \\mu) + ln((2 \\cdot \\pi) ^ \\frac{n} {2} \\cdot \\sqrt{{|\\Sigma|}} \\Bigr ) dx = $$  $$ = \\frac {1} {2} \\cdot E \\Bigl( \\sum_{i,j} (x_i - \\mu_j) \\cdot (\\Sigma)^{-1}_{i,j} \\cdot (x_i - \\mu_j) \\Bigr) + \\frac{1} {2} \\cdot ln ((2 \\cdot \\pi)^n \\cdot |\\Sigma|) = $$   \n",
    "$$ = \\frac {1} {2} \\cdot \\sum_{i,j} \\Bigl( E(  (x_i - \\mu_j) \\cdot (\\Sigma)^{-1}_{i,j} \\cdot (x_i - \\mu_j) )\\Bigr) + \\frac{1} {2} \\cdot ln ((2 \\cdot \\pi)^n \\cdot |\\Sigma|) = $$ $$ = \\frac {1} {2} \\cdot \\sum_i \\sum_j ({\\Sigma)_{i,j}}  \\cdot (\\Sigma)^ {-1}_{i,j} + \\frac{1}{2} \\cdot ln ((2 \\cdot \\pi)^n \\cdot |\\sum|) = $$\n",
    "$$ = \\frac {1} {2} \\cdot \\sum_i \\sum_j ({\\Sigma)_{i,j}}  \\cdot (\\Sigma)^ {-1}_{i,j} + \\frac{1}{2} \\cdot ln ((2 \\cdot \\pi)^n \\cdot |\\Sigma|) = $$\n",
    "$$ = \\frac {1} {2} \\cdot \\sum_i ((\\Sigma)  \\cdot (\\Sigma)^ {-1})_{i,i}+ \\frac{1}{2} \\cdot ln ((2 \\cdot \\pi)^n \\cdot |\\Sigma|) = $$ \n",
    "$$ = \\frac {1} {2} \\cdot \\sum_i (( E)_{i,i}+ \\frac{1}{2} \\cdot ln ((2 \\cdot \\pi)^n \\cdot |\\Sigma|) = $$ \n",
    "$$ = \\frac {n} {2} + \\frac{1}{2} \\cdot ln ((2 \\cdot \\pi)^n \\cdot |\\Sigma|) = $$ \n",
    "$$ = \\frac{1}{2} \\cdot ln ((2 \\cdot \\pi \\cdot e)^n \\cdot |\\Sigma|) = H(S)$$\n",
    "\n",
    "Что и требовалось доказать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
